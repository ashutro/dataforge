[![Docker Hub - Airflow](https://img.shields.io/badge/Docker%20Hub-ashutro%2Fdataforge--airflow-blue)](https://hub.docker.com/r/ashutro/dataforge-airflow)
[![Docker Hub - Jupyter](https://img.shields.io/badge/Docker%20Hub-ashutro%2Fdataforge--jupyter-blue)](https://hub.docker.com/r/ashutro/dataforge-jupyter)
[![Docker Hub - Spark (Bitnami)](https://img.shields.io/badge/Docker%20Hub-bitnami%2Fspark-orange)](https://hub.docker.com/r/bitnami/spark)

DataForge

Docker-first data engineering stack: Spark, Hadoop (HDFS), Kafka + Zookeeper, Airflow, Postgres, MongoDB, and Jupyter â€” all wired together with Docker Compose.

â¸»

âœ¨ What you get
	â€¢	Spark master/worker with web UIs
	â€¢	HDFS NameNode/DataNode
	â€¢	Kafka + Zookeeper
	â€¢	Airflow (LocalExecutor) pre-wired to Postgres & Spark
	â€¢	Postgres + MongoDB with persistent named volumes
	â€¢	Jupyter (PySpark-ready)

Spark uses the official Bitnami image. Airflow and Jupyter are custom images hosted on Docker Hub under ashutro.

â¸»

ğŸ“¦ Docker Hub Images

- `ashutro/dataforge-airflow:1.0` (custom Airflow image)
- `ashutro/dataforge-jupyter:1.0` (custom Jupyter image)
- `bitnami/spark` (official Spark image)

These images extend official bases:
- `ashutro/dataforge-airflow` â†’ based on `apache/airflow:2.9.2` with extra JDK and Python dependencies.
- `ashutro/dataforge-jupyter` â†’ based on `jupyter/base-notebook` (with PySpark support).

This ensures compatibility with official distributions while adding project-specific tooling.

â¸»

ğŸ§° Prerequisites
	â€¢	Docker (Docker Desktop on macOS/Windows; Docker Engine on Linux)
	â€¢	Docker Compose v2 (bundled with Docker Desktop; docker compose version should work)
	â€¢	Git (for cloning the repo)
	â€¢	(Optional) Docker Hub account if you plan to push your own images

â¸»

ğŸš€ Quick Start (local)

# 1) Clone
git clone https://github.com/ashutro/dataforge.git
cd dataforge

# 2) Create your env file
cp .env.example .env
# Edit .env with your credentials/secrets

# 3) (If not pulling from Docker Hub) Build custom images locally
# Only needed if you havenâ€™t pushed your images yet
docker build -t ashutro/dataforge-airflow:1.0 ./airflow
docker build -t ashutro/dataforge-jupyter:1.0 ./jupyter

# 4) Start the stack
docker compose up -d

# 5) Check containers
docker compose ps


â¸»

ğŸŒ Access URLs (localhost)
	â€¢	Airflow Web UI â†’ http://localhost:8082
User/Pass: ${AIRFLOW_ADMIN_USER} / ${AIRFLOW_ADMIN_PASSWORD} (from .env)
	â€¢	Spark Master UI â†’ http://localhost:8080
	â€¢	Spark Worker UI â†’ http://localhost:8081
	â€¢	HDFS NameNode UI â†’ http://localhost:9870
	â€¢	Kafka Broker â†’ localhost:9092 (for host tools on macOS; see notes for Linux/Cloud)
	â€¢	Postgres â†’ localhost:5432  (user: ${POSTGRES_USER}, db: ${POSTGRES_DB})
	â€¢	MongoDB â†’ localhost:27017  (user: ${MONGO_INITDB_ROOT_USERNAME})
	â€¢	Jupyter â†’ http://localhost:8888
First login requires a token printed in logs: docker logs jupyter | grep -m1 token=

â¸»

âš™ï¸ Environment Variables

Copy .env.example to .env and edit values:

# Postgres
POSTGRES_USER=your_user
POSTGRES_PASSWORD=your_password
POSTGRES_DB=dataforge

# Mongo
MONGO_INITDB_ROOT_USERNAME=your_mongo_root
MONGO_INITDB_ROOT_PASSWORD=your_mongo_password

# Airflow admin
AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=admin
AIRFLOW_ADMIN_FIRSTNAME=Ashutosh
AIRFLOW_ADMIN_LASTNAME=Kumar
AIRFLOW_ADMIN_EMAIL=admin@example.com

docker-compose.yml uses these variables for container env and Airflow user creation.

â¸»

ğŸ— Building & Publishing Images (optional)

You only need to build/push Airflow and Jupyter (Spark uses official Bitnami image):

# Build
docker build -t ashutro/dataforge-airflow:1.0 ./airflow
docker build -t ashutro/dataforge-jupyter:1.0 ./jupyter

# Login & push
docker login
docker push ashutro/dataforge-airflow:1.0
docker push ashutro/dataforge-jupyter:1.0

Once published, anyone can run the stack without building locally.

â¸»

ğŸ“¦ Docker Hub Images

The Airflow and Jupyter images are custom builds hosted on Docker Hub under the username ashutro. The Spark image uses the official Bitnami Spark image as its base. You can extend these official bases or use the images directly in your own projects.

ğŸ”½ Pulling the images directly:
```bash
docker pull ashutro/dataforge-airflow:1.0
docker pull ashutro/dataforge-jupyter:1.0
docker pull bitnami/spark:3.5.0
```

â¸»

ğŸ§© Service-to-Service Connection Details
	â€¢	Spark Master URL (inside containers): spark://spark-master:7077
	â€¢	Kafka bootstrap (inside containers): kafka:9092
	â€¢	Kafka bootstrap (host tools on macOS/Windows): host.docker.internal:9092 (as configured)
For Linux or Cloud, see Kafka notes below.
	â€¢	Postgres DSN: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
	â€¢	Mongo URI: mongodb://${MONGO_INITDB_ROOT_USERNAME}:${MONGO_INITDB_ROOT_PASSWORD}@mongo:27017
	â€¢	Airflow is configured with AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077.

â¸»

ğŸ“ˆ Scaling Spark Workers

docker compose up -d --scale spark-worker=3

View workers in the Spark Master UI (http://localhost:8080).

â¸»

â˜ï¸ Run on a Cloud VM (Ubuntu example)
	1.	Create a VM (Ubuntu 22.04). Open security group/firewall for the ports you need (or use SSH tunnels below).
	2.	SSH to the VM and install Docker & Compose:

curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER
newgrp docker


	3.	Clone and configure:

git clone https://github.com/ashutro/dataforge.git
cd dataforge
cp .env.example .env
# edit .env


	4.	(Optional) Login & pull your published images:

docker login
docker compose pull


	5.	Start:

docker compose up -d
docker compose ps


	6.	Access UIs securely using SSH tunnels (recommended):

# On your laptop
ssh -L 8082:localhost:8082 -L 8888:localhost:8888 -L 8080:localhost:8080 \
    -L 9870:localhost:9870 -L 9092:localhost:9092 user@your-vm-ip
# Then open http://localhost:8082, http://localhost:8888, etc.

# On Windows PowerShell (using OpenSSH client)
ssh.exe -L 8082:localhost:8082 -L 8888:localhost:8888 -L 8080:localhost:8080 `
    -L 9870:localhost:9870 -L 9092:localhost:9092 user@your-vm-ip
# Then open http://localhost:8082, http://localhost:8888, etc.

# On Windows with PuTTY
You can also use PuTTY if you prefer a GUI:
- Go to Connection > SSH > Tunnels
- Add the same local ports (8082, 8888, 8080, 9870, 9092) mapped to `localhost:PORT`
- Save and open the session to establish tunnels.


If you must expose ports publicly, ensure proper firewall rules and credentials.

â¸»

ğŸ§  Kafka on Linux/Cloud: advertised listeners

The compose file sets:

KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://host.docker.internal:9092

	â€¢	macOS/Windows host tools â†’ OK.
	â€¢	Linux host tools â†’ host.docker.internal may not resolve. Use either:
	â€¢	inter-container only: PLAINTEXT://kafka:9092
	â€¢	external clients: PLAINTEXT://<HOST_PUBLIC_IP>:9092
Update kafka service env in docker-compose.yml accordingly when running on Linux or Cloud.

â¸»

ğŸ§ª Verifying the stack

# Check logs
docker compose logs -f airflow

# Jupyter token
docker logs jupyter | grep -m1 token=

# Postgres connectivity from host
psql postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:5432/${POSTGRES_DB}


â¸»

ğŸ›‘ Stop / Reset

# Stop but keep data
docker compose down

# Stop AND remove named volumes (data loss for Postgres/Mongo/HDFS!)
docker compose down -v


â¸»

ğŸ§° Troubleshooting
	â€¢	Port already in use â†’ Change the host port in docker-compose.yml or free the port.
	â€¢	Apple Silicon  â†’ Hadoop images run under linux/amd64 emulation. Expect overhead; allocate enough RAM/CPU in Docker Desktop.
	â€¢	Airflow user creation repeats â†’ The command tolerates existing user (|| true). Check airflow logs if webserver/scheduler donâ€™t start.
	â€¢	Kafka clients canâ€™t connect â†’ Fix KAFKA_CFG_ADVERTISED_LISTENERS for your environment (see Kafka section).
	â€¢	Permission on Docker socket (Linux) â†’ If Airflow needs Docker access via /var/run/docker.sock, ensure your user is in the docker group, or run with sudo.
	â€¢	Windows WSL2 networking â†’ On Windows with WSL2 backend, `localhost` works for exposed ports. If containers canâ€™t connect to each other via `host.docker.internal`, update the compose file to use service names (e.g., `kafka:9092`) instead.

â¸»

ğŸ”„ Upgrading & Version pinning
	â€¢	Change image tags in docker-compose.yml (e.g., :1.1 or :3.5.0).
	â€¢	Then:

docker compose pull
docker compose up -d


â¸»

ğŸ“š Folder structure (suggested)

.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ .env.example  -> copy to .env
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â””â”€â”€ jupyter/
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ requirements.txt


â¸»

ğŸ“ Credits
	â€¢	Spark image by Bitnami
	â€¢	Hadoop images by bde2020
	â€¢	Kafka/Zookeeper images by Bitnami
	â€¢	Airflow by Apache Airflow
	â€¢	Jupyter images by Project Jupyter

â¸»

ğŸ“„ License

Choose a license (e.g., MIT) and add a LICENSE file if publishing publicly.
